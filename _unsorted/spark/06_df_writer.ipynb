{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrameWriter\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter\n",
    "\n",
    "    df.write.format(\"csv\")\n",
    "    .option(\"mode\", \"OVERWRITE\") # append, overwrite, errorIfExists, ignore\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"path\", \"path/to/file(s)\")\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "      .parquet('data/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write CSV\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(path='write/out.csv', \n",
    "             mode='overwrite',\n",
    "             compression='bzip2',\n",
    "             sep=',',\n",
    "             header='true',\n",
    "             dateFormat='yyyy-MM-dd',\n",
    "             encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write JSON\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(path='write/out.json', \n",
    "              mode='overwrite', # append, overwrite, ignore, error or errorifexists (default)\n",
    "              compression='gzip',\n",
    "              dateFormat='yyyy-MM-dd',\n",
    "              encoding='UTF-8',\n",
    "              lineSep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write ORC\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.orc(path='write/out.orc',\n",
    "             mode='overwrite',\n",
    "             partitionBy='DEST_COUNTRY_NAME',\n",
    "             compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Parquet\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(path='write/out.parquet',\n",
    "                mode='overwrite',\n",
    "                partitionBy='DEST_COUNTRY_NAME',\n",
    "                compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Text\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "# supports only for one column and in form of string\n",
    "(df.selectExpr(\"DEST_COUNTRY_NAME\", \"cast(count as string) count\")\n",
    ".write\n",
    ".partitionBy(\"count\")\n",
    ".mode('overwrite')\n",
    ".text(path='write/out.txt')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Table\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.saveAsTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.saveAsTable(name='test_table',\n",
    "                      format='parquet',\n",
    "                      mode='overwrite'))\n",
    "\n",
    "# for example in HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-b4945019-12d4-4561-8a49-a1d3bfc6d11c-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls spark-warehouse/test_table/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').format('csv').saveAsTable('my_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-fe18ec64-e00a-4bbe-a880-6058cc532ae9-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! ls spark-warehouse/my_table/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a specific Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyfiles/avro_write.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyfiles/avro_write.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# prepare data\n",
    "df = (spark\n",
    "      .read\n",
    "      .parquet('data/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet'))\n",
    "\n",
    "# write data\n",
    "(df.write\n",
    " .format('avro')\n",
    ".mode('overwrite')\n",
    ".save('write/out.avro'))\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/esn/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/esn/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/Users/esn/anaconda3/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-avro_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-af76dc26-83a1-407b-9322-cd0480c9fbc9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.11;2.4.3 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.3/spark-avro_2.11-2.4.3.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.11;2.4.3!spark-avro_2.11.jar (159ms)\n",
      ":: resolution report :: resolve 1970ms :: artifacts dl 164ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.11;2.4.3 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   1   |   1   |   0   ||   2   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-af76dc26-83a1-407b-9322-cd0480c9fbc9\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 1 already retrieved (182kB/6ms)\n",
      "19/09/29 23:06:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/09/29 23:06:36 INFO SparkContext: Running Spark version 2.4.4\n",
      "19/09/29 23:06:36 INFO SparkContext: Submitted application: Spark session\n",
      "19/09/29 23:06:36 INFO SecurityManager: Changing view acls to: esn\n",
      "19/09/29 23:06:36 INFO SecurityManager: Changing modify acls to: esn\n",
      "19/09/29 23:06:36 INFO SecurityManager: Changing view acls groups to: \n",
      "19/09/29 23:06:36 INFO SecurityManager: Changing modify acls groups to: \n",
      "19/09/29 23:06:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(esn); groups with view permissions: Set(); users  with modify permissions: Set(esn); groups with modify permissions: Set()\n",
      "19/09/29 23:06:42 INFO Utils: Successfully started service 'sparkDriver' on port 65300.\n",
      "19/09/29 23:06:42 INFO SparkEnv: Registering MapOutputTracker\n",
      "19/09/29 23:06:42 INFO SparkEnv: Registering BlockManagerMaster\n",
      "19/09/29 23:06:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/09/29 23:06:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/09/29 23:06:42 INFO DiskBlockManager: Created local directory at /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/blockmgr-09f09537-8bc5-4f03-ac2e-fd0d2c2d2666\n",
      "19/09/29 23:06:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/09/29 23:06:42 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "19/09/29 23:06:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "19/09/29 23:06:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "19/09/29 23:06:42 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "19/09/29 23:06:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.3.105:4042\n",
      "19/09/29 23:06:42 INFO SparkContext: Added JAR file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar at spark://192.168.3.105:65300/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791202698\n",
      "19/09/29 23:06:42 INFO SparkContext: Added JAR file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://192.168.3.105:65300/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791202699\n",
      "19/09/29 23:06:42 INFO SparkContext: Added file file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar at file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791202756\n",
      "19/09/29 23:06:42 INFO Utils: Copying /Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.apache.spark_spark-avro_2.11-2.4.3.jar\n",
      "19/09/29 23:06:42 INFO SparkContext: Added file file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791202783\n",
      "19/09/29 23:06:42 INFO Utils: Copying /Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.spark-project.spark_unused-1.0.0.jar\n",
      "19/09/29 23:06:42 INFO Executor: Starting executor ID driver on host localhost\n",
      "19/09/29 23:06:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65301.\n",
      "19/09/29 23:06:43 INFO NettyBlockTransferService: Server created on 192.168.3.105:65301\n",
      "19/09/29 23:06:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/09/29 23:06:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.3.105, 65301, None)\n",
      "19/09/29 23:06:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.3.105:65301 with 366.3 MB RAM, BlockManagerId(driver, 192.168.3.105, 65301, None)\n",
      "19/09/29 23:06:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.3.105, 65301, None)\n",
      "19/09/29 23:06:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.3.105, 65301, None)\n",
      "19/09/29 23:06:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/esn/repos/awesomepy-notebooks/spark/spark-warehouse/').\n",
      "19/09/29 23:06:43 INFO SharedState: Warehouse path is 'file:/Users/esn/repos/awesomepy-notebooks/spark/spark-warehouse/'.\n",
      "19/09/29 23:06:44 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "19/09/29 23:06:45 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "19/09/29 23:06:45 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "19/09/29 23:06:45 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "19/09/29 23:06:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "19/09/29 23:06:45 INFO DAGScheduler: Missing parents: List()\n",
      "19/09/29 23:06:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/09/29 23:06:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 70.9 KB, free 366.2 MB)\n",
      "19/09/29 23:06:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.2 KB, free 366.2 MB)\n",
      "19/09/29 23:06:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.3.105:65301 (size: 25.2 KB, free: 366.3 MB)\n",
      "19/09/29 23:06:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161\n",
      "19/09/29 23:06:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "19/09/29 23:06:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "19/09/29 23:06:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8080 bytes)\n",
      "19/09/29 23:06:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "19/09/29 23:06:45 INFO Executor: Fetching file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791202756\n",
      "19/09/29 23:06:45 INFO Utils: /Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.apache.spark_spark-avro_2.11-2.4.3.jar\n",
      "19/09/29 23:06:45 INFO Executor: Fetching file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791202783\n",
      "19/09/29 23:06:45 INFO Utils: /Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.spark-project.spark_unused-1.0.0.jar\n",
      "19/09/29 23:06:45 INFO Executor: Fetching spark://192.168.3.105:65300/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791202698\n",
      "19/09/29 23:06:45 INFO TransportClientFactory: Successfully created connection to /192.168.3.105:65300 after 74 ms (0 ms spent in bootstraps)\n",
      "19/09/29 23:06:45 INFO Utils: Fetching spark://192.168.3.105:65300/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/fetchFileTemp6295783885559538989.tmp\n",
      "19/09/29 23:06:45 INFO Utils: /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/fetchFileTemp6295783885559538989.tmp has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.apache.spark_spark-avro_2.11-2.4.3.jar\n",
      "19/09/29 23:06:45 INFO Executor: Adding file:/private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.apache.spark_spark-avro_2.11-2.4.3.jar to class loader\n",
      "19/09/29 23:06:45 INFO Executor: Fetching spark://192.168.3.105:65300/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791202699\n",
      "19/09/29 23:06:45 INFO Utils: Fetching spark://192.168.3.105:65300/jars/org.spark-project.spark_unused-1.0.0.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/fetchFileTemp4481584501256933638.tmp\n",
      "19/09/29 23:06:45 INFO Utils: /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/fetchFileTemp4481584501256933638.tmp has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.spark-project.spark_unused-1.0.0.jar\n",
      "19/09/29 23:06:45 INFO Executor: Adding file:/private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/userFiles-6e432995-76ca-486e-96a3-d704056343ea/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
      "19/09/29 23:06:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1691 bytes result sent to driver\n",
      "19/09/29 23:06:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 678 ms on localhost (executor driver) (1/1)\n",
      "19/09/29 23:06:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/09/29 23:06:46 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.905 s\n",
      "19/09/29 23:06:46 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.982361 s\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 20\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 1\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 21\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 19\n",
      "19/09/29 23:06:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.3.105:65301 in memory (size: 25.2 KB, free: 366.3 MB)\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 24\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 6\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 12\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 7\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 25\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 16\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 4\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 3\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 22\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 13\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 10\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 5\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 17\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 14\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 18\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 2\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 15\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 11\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 23\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 8\n",
      "19/09/29 23:06:46 INFO ContextCleaner: Cleaned accumulator 9\n",
      "19/09/29 23:06:48 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/09/29 23:06:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "19/09/29 23:06:48 INFO FileSourceStrategy: Output Data Schema: struct<DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint ... 1 more fields>\n",
      "19/09/29 23:06:48 INFO FileSourceScanExec: Pushed Filters: \n",
      "19/09/29 23:06:48 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "19/09/29 23:06:48 INFO AvroFileFormat: Compressing Avro output using the snappy codec\n",
      "19/09/29 23:06:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/09/29 23:06:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/09/29 23:06:49 INFO CodeGenerator: Code generated in 246.514122 ms\n",
      "19/09/29 23:06:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 281.5 KB, free 366.0 MB)\n",
      "19/09/29 23:06:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.2 KB, free 366.0 MB)\n",
      "19/09/29 23:06:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.3.105:65301 (size: 24.2 KB, free: 366.3 MB)\n",
      "19/09/29 23:06:49 INFO SparkContext: Created broadcast 1 from save at NativeMethodAccessorImpl.java:0\n",
      "19/09/29 23:06:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198225 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/09/29 23:06:49 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "19/09/29 23:06:49 INFO DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "19/09/29 23:06:49 INFO DAGScheduler: Final stage: ResultStage 1 (save at NativeMethodAccessorImpl.java:0)\n",
      "19/09/29 23:06:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "19/09/29 23:06:49 INFO DAGScheduler: Missing parents: List()\n",
      "19/09/29 23:06:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/09/29 23:06:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 150.0 KB, free 365.9 MB)\n",
      "19/09/29 23:06:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 53.8 KB, free 365.8 MB)\n",
      "19/09/29 23:06:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.3.105:65301 (size: 53.8 KB, free: 366.2 MB)\n",
      "19/09/29 23:06:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161\n",
      "19/09/29 23:06:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "19/09/29 23:06:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "19/09/29 23:06:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8331 bytes)\n",
      "19/09/29 23:06:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "19/09/29 23:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/09/29 23:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/09/29 23:06:49 INFO FileScanRDD: Reading File path: file:///Users/esn/repos/awesomepy-notebooks/spark/data/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet, range: 0-3921, partition values: [empty row]\n",
      "19/09/29 23:06:50 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "19/09/29 23:06:50 INFO FileOutputCommitter: Saved output of task 'attempt_20190929230649_0001_m_000000_1' to file:/Users/esn/repos/awesomepy-notebooks/spark/write/out.avro/_temporary/0/task_20190929230649_0001_m_000000\n",
      "19/09/29 23:06:50 INFO SparkHadoopMapRedUtil: attempt_20190929230649_0001_m_000000_1: Committed\n",
      "19/09/29 23:06:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2293 bytes result sent to driver\n",
      "19/09/29 23:06:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 949 ms on localhost (executor driver) (1/1)\n",
      "19/09/29 23:06:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "19/09/29 23:06:50 INFO DAGScheduler: ResultStage 1 (save at NativeMethodAccessorImpl.java:0) finished in 1.018 s\n",
      "19/09/29 23:06:50 INFO DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 1.025415 s\n",
      "19/09/29 23:06:50 INFO FileFormatWriter: Write Job 84b78d85-c165-4f0e-a539-54b617335150 committed.\n",
      "19/09/29 23:06:50 INFO FileFormatWriter: Finished processing stats for write job 84b78d85-c165-4f0e-a539-54b617335150.\n",
      "19/09/29 23:06:50 INFO SparkUI: Stopped Spark web UI at http://192.168.3.105:4042\n",
      "19/09/29 23:06:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "19/09/29 23:06:50 INFO MemoryStore: MemoryStore cleared\n",
      "19/09/29 23:06:50 INFO BlockManager: BlockManager stopped\n",
      "19/09/29 23:06:50 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "19/09/29 23:06:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "19/09/29 23:06:50 INFO SparkContext: Successfully stopped SparkContext\n",
      "19/09/29 23:06:51 INFO ShutdownHookManager: Shutdown hook called\n",
      "19/09/29 23:06:51 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034/pyspark-76c40460-4b70-4640-9938-f6bf3a24db25\n",
      "19/09/29 23:06:51 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-f3861c00-c28a-43d7-aaa4-f9921749c034\n",
      "19/09/29 23:06:51 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-b4e7204a-477a-41c7-ac6c-1b99abdbedfe\n"
     ]
    }
   ],
   "source": [
    "! spark-submit \\\n",
    "--packages org.apache.spark:spark-avro_2.11:2.4.3 \\\n",
    "pyfiles/avro_write.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write N Separate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(5).write.mode('overwrite').format('csv').save('write/repartitioned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-9d4a5a81-94ca-479c-a0db-d15b72baa2d8-c000.csv\n",
      "part-00001-9d4a5a81-94ca-479c-a0db-d15b72baa2d8-c000.csv\n",
      "part-00002-9d4a5a81-94ca-479c-a0db-d15b72baa2d8-c000.csv\n",
      "part-00003-9d4a5a81-94ca-479c-a0db-d15b72baa2d8-c000.csv\n",
      "part-00004-9d4a5a81-94ca-479c-a0db-d15b72baa2d8-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! ls write/repartitioned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "Can parallelize the reading. Drawback: to little files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').format('csv').partitionBy('DEST_COUNTRY_NAME').save('write/partitioned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEST_COUNTRY_NAME=Afghanistan\n",
      "DEST_COUNTRY_NAME=Angola\n",
      "DEST_COUNTRY_NAME=Anguilla\n",
      "DEST_COUNTRY_NAME=Antigua and Barbuda\n",
      "DEST_COUNTRY_NAME=Argentina\n",
      "DEST_COUNTRY_NAME=Aruba\n",
      "DEST_COUNTRY_NAME=Australia\n",
      "DEST_COUNTRY_NAME=Austria\n",
      "DEST_COUNTRY_NAME=Azerbaijan\n",
      "DEST_COUNTRY_NAME=Bahrain\n"
     ]
    }
   ],
   "source": [
    "! ls write/partitioned.csv | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing\n",
    "This can help avoid shuffles. You can define large enough for HDFS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be only use with tables\n",
    "num_buckets = 10\n",
    "df.write.mode('overwrite').format('csv').bucketBy(num_buckets, 'DEST_COUNTRY_NAME').saveAsTable('bucketedcsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00000.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00001.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00002.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00003.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00004.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00005.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00006.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00007.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00008.c000.csv\n",
      "part-00000-1ce6aae1-501c-4eb1-b7e1-afa618512f68_00009.c000.csv\n"
     ]
    }
   ],
   "source": [
    "! ls spark-warehouse/bucketedcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
