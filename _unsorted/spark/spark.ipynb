{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to install\n",
    "\n",
    "    pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Manager\n",
    "\n",
    "    localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "    pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark The Definitive Guide\n",
    "\n",
    "https://github.com/databricks/Spark-The-Definitive-Guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databrics Sertification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"700\"\n",
       "            src=\"databricks_sertification.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10e1130f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### from IPython.display import IFrame\n",
    "IFrame(\"databricks_sertification.pdf\", width=1100, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data =  (spark.read\n",
    "                .option(\"inferShema\", \"true\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(\"data/2010-summary.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "flight_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count='264'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count='69'),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count='24'),\n",
       " Row(DEST_COUNTRY_NAME='Equatorial Guinea', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count='25'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count='54'),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count='477'),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count='29'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Marshall Islands', count='44')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 10 rows\n",
    "flight_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [DEST_COUNTRY_NAME#23,ORIGIN_COUNTRY_NAME#24,count#25] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/esn/repos/awesomepy-notebooks/spark/data/2010-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:string>\n"
     ]
    }
   ],
   "source": [
    "# explain\n",
    "flight_data.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Algeria', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Malaysia', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Azerbaijan', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Equatorial Guinea', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Liberia', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Vietnam', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Slovakia', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bosnia and Herzegovina', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Serbia', count='1')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort\n",
    "flight_data.sort(\"count\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to table to use the normal sql\n",
    "flight_data.createOrReplaceTempView(\"flight_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|count|\n",
      "+--------------------+-----+\n",
      "|       United States|  131|\n",
      "|British Virgin Is...|    1|\n",
      "|              Russia|    1|\n",
      "|            Paraguay|    1|\n",
      "|             Senegal|    1|\n",
      "|              Sweden|    1|\n",
      "|            Kiribati|    1|\n",
      "|              Guyana|    1|\n",
      "|         Philippines|    1|\n",
      "|            Malaysia|    1|\n",
      "|           Singapore|    1|\n",
      "|                Fiji|    1|\n",
      "|              Turkey|    1|\n",
      "|             Germany|    1|\n",
      "|         Afghanistan|    1|\n",
      "|              Jordan|    1|\n",
      "|               Palau|    1|\n",
      "|Turks and Caicos ...|    1|\n",
      "|              France|    1|\n",
      "|              Greece|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how to make select operation\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1) as count\n",
    "FROM flight_data_table\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|count|\n",
      "+--------------------+-----+\n",
      "|       United States|  131|\n",
      "|British Virgin Is...|    1|\n",
      "|              Russia|    1|\n",
      "|            Paraguay|    1|\n",
      "|             Senegal|    1|\n",
      "|              Sweden|    1|\n",
      "|            Kiribati|    1|\n",
      "|              Guyana|    1|\n",
      "|         Philippines|    1|\n",
      "|            Malaysia|    1|\n",
      "|           Singapore|    1|\n",
      "|                Fiji|    1|\n",
      "|              Turkey|    1|\n",
      "|             Germany|    1|\n",
      "|         Afghanistan|    1|\n",
      "|              Jordan|    1|\n",
      "|               Palau|    1|\n",
      "|Turks and Caicos ...|    1|\n",
      "|              France|    1|\n",
      "|              Greece|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the same but using df\n",
    "from pyspark.sql.functions import desc\n",
    "flight_data.groupBy(\"DEST_COUNTRY_NAME\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT col1, func(col1) as new_col\n",
    "# FROM table1\n",
    "# WHERE expr\n",
    "# JOIN table2\n",
    "# ON table1.col1 = table2.col1\n",
    "# GROUP BY col1\n",
    "# ORDER BY new_col\n",
    "# HAVING expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "values1 = [(1, 'sport'),(2, 'casual'),(3, 'urban'),(4, 'modern'), (5, 'modern')]\n",
    "df1 = spark.createDataFrame(values1,['id', 'style'])\n",
    " \n",
    "values2 = [(2, 'jeans'),(3, 't-shirt'),(4, 'shoues'),(5, 'glasses')]\n",
    "df2 = spark.createDataFrame(values2,['id', 'assortment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id| style|assortment|\n",
      "+---+------+----------+\n",
      "|  3| urban|     jeans|\n",
      "|  4|modern|   t-shirt|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join = df1.join(df2, df1.id == df2.id)\n",
    "inner_join.select([df1.id, 'style', 'assortment']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id| style|assortment|\n",
      "+---+------+----------+\n",
      "|  5|modern|   glasses|\n",
      "|  2|casual|     jeans|\n",
      "|  4|modern|    shoues|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df1\n",
    " .join(df2, df1.id == df2.id)\n",
    " .select([df1.id, 'style', 'assortment'])\n",
    " .where(df1.style != 'urban')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sel_style|count|\n",
      "+---------+-----+\n",
      "|   modern|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# SELECT \n",
    "# df1.style as sel_style,\n",
    "# count(df1.style) as count\n",
    "# FROM df1\n",
    "# JOIN df2\n",
    "# ON df1.id == df2.id \n",
    "# GROUP BY style\n",
    "# ORDER BY style DESC\n",
    "# WHERE df1.style <> 'urban'\n",
    "# HANVING count > 1\n",
    "\n",
    "\n",
    "(df1                                   \n",
    " .join(df2, df1.id == df2.id)          \n",
    " .groupBy('style')                     \n",
    " .agg(count('style').alias('count'))  # same as .count()\n",
    " .filter(column('count') > 1)  \n",
    " .withColumnRenamed('style', 'sel_style')\n",
    " .where(df1.style != 'urban')\n",
    " .sort(desc('count'))\n",
    " # .explain()\n",
    " .show())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
