{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data for the “core” data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)\n",
    "\n",
    "    read.csv\n",
    "    read.json\n",
    "    read.parquet\n",
    "    read.orc\n",
    "    read.text\n",
    "    read.table\n",
    "    read.jdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# csv\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType()),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType()),\n",
    "    StructField(\"count\", IntegerType())\n",
    "])\n",
    "\n",
    "# or so\n",
    "# schema = \"DESTINATION VARCHAR(255), ORIGIN VARCHAR(255), NUM_OF_FLIGHTS DOUBLE\"\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .option(\"inferShema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .schema(schema)\n",
    "      .csv('data/2010-summary.csv'))  # json, parquet, ocr\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'int')]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same with fomat load\n",
    "df = (spark\n",
    "      .read\n",
    "      .format('csv')\n",
    "      .option(\"inferShema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load('data/2010-summary.csv'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json\n",
    "# works perfectly without options\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .json('data/2010-summary.json'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'bigint')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same with \n",
    "df = (spark\n",
    "      .read\n",
    "      .format('json')\n",
    "      .load('data/2010-summary.json'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "The default format for Spark and the best ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parquet. The best format ever\n",
    "# self defined format\n",
    "# works perfectly without options\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .parquet('data/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'bigint')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC\n",
    "\n",
    "Hadoop format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orc\n",
    "# works perfectly without options\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .orc('data/part-r-00000-2c4f7d96-e703-4de3-af1b-1441d172c80f.snappy.orc'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'bigint')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text File\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "| 0::2::3::1424380312|\n",
      "| 0::3::1::1424380312|\n",
      "| 0::5::2::1424380312|\n",
      "| 0::9::4::1424380312|\n",
      "|0::11::1::1424380312|\n",
      "|0::12::2::1424380312|\n",
      "|0::15::1::1424380312|\n",
      "|0::17::1::1424380312|\n",
      "|0::19::1::1424380312|\n",
      "|0::21::1::1424380312|\n",
      "|0::23::1::1424380312|\n",
      "|0::26::3::1424380312|\n",
      "|0::27::1::1424380312|\n",
      "|0::28::1::1424380312|\n",
      "|0::29::1::1424380312|\n",
      "|0::30::1::1424380312|\n",
      "|0::31::1::1424380312|\n",
      "|0::34::1::1424380312|\n",
      "|0::37::1::1424380312|\n",
      "|0::41::2::1424380312|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "      .text('data/sample_movielens_ratings.txt'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+\n",
      "|col1|col2|col3|      col4|\n",
      "+----+----+----+----------+\n",
      "|   0|   2|   3|1424380312|\n",
      "|   0|   3|   1|1424380312|\n",
      "|   0|   5|   2|1424380312|\n",
      "|   0|   9|   4|1424380312|\n",
      "|   0|  11|   1|1424380312|\n",
      "|   0|  12|   2|1424380312|\n",
      "|   0|  15|   1|1424380312|\n",
      "|   0|  17|   1|1424380312|\n",
      "|   0|  19|   1|1424380312|\n",
      "|   0|  21|   1|1424380312|\n",
      "|   0|  23|   1|1424380312|\n",
      "|   0|  26|   3|1424380312|\n",
      "|   0|  27|   1|1424380312|\n",
      "|   0|  28|   1|1424380312|\n",
      "|   0|  29|   1|1424380312|\n",
      "|   0|  30|   1|1424380312|\n",
      "|   0|  31|   1|1424380312|\n",
      "|   0|  34|   1|1424380312|\n",
      "|   0|  37|   1|1424380312|\n",
      "|   0|  41|   2|1424380312|\n",
      "+----+----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "(df.withColumn(\"col1\", split(col(\"value\"), \"::\").getItem(0))\n",
    " .withColumn(\"col2\", split(col(\"value\"), \"::\").getItem(1))\n",
    " .withColumn(\"col3\", split(col(\"value\"), \"::\").getItem(2))\n",
    " .withColumn(\"col4\", split(col(\"value\"), \"::\").getItem(3))\n",
    " .drop(\"value\")\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+\n",
      "| _1| _2| _3|        _4|\n",
      "+---+---+---+----------+\n",
      "|  0|  2|  3|1424380312|\n",
      "|  0|  3|  1|1424380312|\n",
      "|  0|  5|  2|1424380312|\n",
      "|  0|  9|  4|1424380312|\n",
      "|  0| 11|  1|1424380312|\n",
      "|  0| 12|  2|1424380312|\n",
      "|  0| 15|  1|1424380312|\n",
      "|  0| 17|  1|1424380312|\n",
      "|  0| 19|  1|1424380312|\n",
      "|  0| 21|  1|1424380312|\n",
      "|  0| 23|  1|1424380312|\n",
      "|  0| 26|  3|1424380312|\n",
      "|  0| 27|  1|1424380312|\n",
      "|  0| 28|  1|1424380312|\n",
      "|  0| 29|  1|1424380312|\n",
      "|  0| 30|  1|1424380312|\n",
      "|  0| 31|  1|1424380312|\n",
      "|  0| 34|  1|1424380312|\n",
      "|  0| 37|  1|1424380312|\n",
      "|  0| 41|  2|1424380312|\n",
      "+---+---+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using rdd\n",
    "rdd = (spark.sparkContext.textFile('data/sample_movielens_ratings.txt'))\n",
    "\n",
    "df = rdd.map(lambda x: x.split(\"::\")).toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "      .parquet('data/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet'))\n",
    "df.createOrReplaceTempView('tmpTable')\n",
    "\n",
    "###\n",
    "spark.read.table('tmpTable').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read non-core formats\n",
    "\n",
    "See for example how to read AVRO:\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-avro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pyfiles/avro_read.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyfiles/avro_read.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.read.format(\"avro\").load(\"data/userdata1.avro\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/esn/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/esn/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/Users/esn/anaconda3/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-avro_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-beb05846-0d02-4f34-83ec-c5fbe70be759;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.11;2.4.3 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 349ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.11;2.4.3 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-beb05846-0d02-4f34-83ec-c5fbe70be759\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/28ms)\n",
      "19/09/29 23:07:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/09/29 23:07:58 INFO SparkContext: Running Spark version 2.4.4\n",
      "19/09/29 23:07:58 INFO SparkContext: Submitted application: Spark session\n",
      "19/09/29 23:07:58 INFO SecurityManager: Changing view acls to: esn\n",
      "19/09/29 23:07:58 INFO SecurityManager: Changing modify acls to: esn\n",
      "19/09/29 23:07:58 INFO SecurityManager: Changing view acls groups to: \n",
      "19/09/29 23:07:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "19/09/29 23:07:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(esn); groups with view permissions: Set(); users  with modify permissions: Set(esn); groups with modify permissions: Set()\n",
      "19/09/29 23:08:04 INFO Utils: Successfully started service 'sparkDriver' on port 65306.\n",
      "19/09/29 23:08:04 INFO SparkEnv: Registering MapOutputTracker\n",
      "19/09/29 23:08:04 INFO SparkEnv: Registering BlockManagerMaster\n",
      "19/09/29 23:08:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/09/29 23:08:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/09/29 23:08:04 INFO DiskBlockManager: Created local directory at /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/blockmgr-a57c3dde-2300-43f9-b11c-db6723032c43\n",
      "19/09/29 23:08:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/09/29 23:08:04 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "19/09/29 23:08:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "19/09/29 23:08:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "19/09/29 23:08:04 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "19/09/29 23:08:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.3.105:4042\n",
      "19/09/29 23:08:04 INFO SparkContext: Added JAR file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar at spark://192.168.3.105:65306/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791284533\n",
      "19/09/29 23:08:04 INFO SparkContext: Added JAR file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://192.168.3.105:65306/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791284534\n",
      "19/09/29 23:08:04 INFO SparkContext: Added file file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar at file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791284588\n",
      "19/09/29 23:08:04 INFO Utils: Copying /Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.apache.spark_spark-avro_2.11-2.4.3.jar\n",
      "19/09/29 23:08:04 INFO SparkContext: Added file file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791284622\n",
      "19/09/29 23:08:04 INFO Utils: Copying /Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.spark-project.spark_unused-1.0.0.jar\n",
      "19/09/29 23:08:04 INFO Executor: Starting executor ID driver on host localhost\n",
      "19/09/29 23:08:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65307.\n",
      "19/09/29 23:08:04 INFO NettyBlockTransferService: Server created on 192.168.3.105:65307\n",
      "19/09/29 23:08:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/09/29 23:08:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.3.105, 65307, None)\n",
      "19/09/29 23:08:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.3.105:65307 with 366.3 MB RAM, BlockManagerId(driver, 192.168.3.105, 65307, None)\n",
      "19/09/29 23:08:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.3.105, 65307, None)\n",
      "19/09/29 23:08:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.3.105, 65307, None)\n",
      "19/09/29 23:08:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/esn/repos/awesomepy-notebooks/spark/spark-warehouse/').\n",
      "19/09/29 23:08:05 INFO SharedState: Warehouse path is 'file:/Users/esn/repos/awesomepy-notebooks/spark/spark-warehouse/'.\n",
      "19/09/29 23:08:06 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "19/09/29 23:08:09 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/09/29 23:08:09 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "19/09/29 23:08:09 INFO FileSourceStrategy: Output Data Schema: struct<registration_dttm: string, id: bigint, first_name: string, last_name: string, email: string ... 11 more fields>\n",
      "19/09/29 23:08:09 INFO FileSourceScanExec: Pushed Filters: \n",
      "19/09/29 23:08:09 INFO CodeGenerator: Code generated in 402.031339 ms\n",
      "19/09/29 23:08:10 INFO CodeGenerator: Code generated in 58.929925 ms\n",
      "19/09/29 23:08:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 278.9 KB, free 366.0 MB)\n",
      "19/09/29 23:08:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.6 KB, free 366.0 MB)\n",
      "19/09/29 23:08:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.3.105:65307 (size: 23.6 KB, free: 366.3 MB)\n",
      "19/09/29 23:08:10 INFO SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\n",
      "19/09/29 23:08:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4287865 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/09/29 23:08:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "19/09/29 23:08:10 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "19/09/29 23:08:10 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "19/09/29 23:08:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "19/09/29 23:08:10 INFO DAGScheduler: Missing parents: List()\n",
      "19/09/29 23:08:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/09/29 23:08:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KB, free 366.0 MB)\n",
      "19/09/29 23:08:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KB, free 366.0 MB)\n",
      "19/09/29 23:08:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.3.105:65307 (size: 5.4 KB, free: 366.3 MB)\n",
      "19/09/29 23:08:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "19/09/29 23:08:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "19/09/29 23:08:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "19/09/29 23:08:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8285 bytes)\n",
      "19/09/29 23:08:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "19/09/29 23:08:10 INFO Executor: Fetching file:///Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791284588\n",
      "19/09/29 23:08:10 INFO Utils: /Users/esn/.ivy2/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.apache.spark_spark-avro_2.11-2.4.3.jar\n",
      "19/09/29 23:08:10 INFO Executor: Fetching file:///Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791284622\n",
      "19/09/29 23:08:10 INFO Utils: /Users/esn/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.spark-project.spark_unused-1.0.0.jar\n",
      "19/09/29 23:08:10 INFO Executor: Fetching spark://192.168.3.105:65306/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar with timestamp 1569791284533\n",
      "19/09/29 23:08:10 INFO TransportClientFactory: Successfully created connection to /192.168.3.105:65306 after 72 ms (0 ms spent in bootstraps)\n",
      "19/09/29 23:08:10 INFO Utils: Fetching spark://192.168.3.105:65306/jars/org.apache.spark_spark-avro_2.11-2.4.3.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/fetchFileTemp1375254704194608085.tmp\n",
      "19/09/29 23:08:11 INFO Utils: /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/fetchFileTemp1375254704194608085.tmp has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.apache.spark_spark-avro_2.11-2.4.3.jar\n",
      "19/09/29 23:08:11 INFO Executor: Adding file:/private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.apache.spark_spark-avro_2.11-2.4.3.jar to class loader\n",
      "19/09/29 23:08:11 INFO Executor: Fetching spark://192.168.3.105:65306/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1569791284534\n",
      "19/09/29 23:08:11 INFO Utils: Fetching spark://192.168.3.105:65306/jars/org.spark-project.spark_unused-1.0.0.jar to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/fetchFileTemp5403731714215959945.tmp\n",
      "19/09/29 23:08:11 INFO Utils: /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/fetchFileTemp5403731714215959945.tmp has been previously copied to /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.spark-project.spark_unused-1.0.0.jar\n",
      "19/09/29 23:08:11 INFO Executor: Adding file:/private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/userFiles-ed7302fb-974e-4014-895f-aa1e212fc606/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
      "19/09/29 23:08:11 INFO FileScanRDD: Reading File path: file:///Users/esn/repos/awesomepy-notebooks/spark/data/userdata1.avro, range: 0-93561, partition values: [empty row]\n",
      "19/09/29 23:08:11 INFO CodeGenerator: Code generated in 107.060963 ms\n",
      "19/09/29 23:08:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4853 bytes result sent to driver\n",
      "19/09/29 23:08:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 829 ms on localhost (executor driver) (1/1)\n",
      "19/09/29 23:08:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/09/29 23:08:11 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.022 s\n",
      "19/09/29 23:08:11 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.116264 s\n",
      "+--------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|   registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
      "+--------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|2016-02-03T07:55:29Z|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
      "|2016-02-03T17:04:03Z|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|               null|              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
      "|2016-02-03T01:09:31Z|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
      "|2016-02-03T12:36:21Z|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
      "|2016-02-03T05:05:31Z|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     null|                    |                    |\n",
      "|2016-02-03T07:22:34Z|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
      "|2016-02-03T08:33:08Z|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
      "|2016-02-03T06:47:06Z|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|               null|Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
      "|2016-02-03T03:52:53Z|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|               null|         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
      "|2016-02-03T18:29:47Z| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
      "|2016-02-03T12:10:42Z| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
      "|2016-02-03T18:04:34Z| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
      "|2016-02-03T18:48:17Z| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
      "|2016-02-03T21:46:52Z| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
      "|2016-02-03T08:53:23Z| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
      "|2016-02-03T00:44:01Z| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
      "|2016-02-03T00:57:45Z| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
      "|2016-02-03T16:44:24Z| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
      "|2016-02-03T11:45:54Z| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
      "|2016-02-03T10:30:36Z| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|               null|               China|          |137251.19|                    |                    |\n",
      "+--------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "19/09/29 23:08:11 INFO SparkUI: Stopped Spark web UI at http://192.168.3.105:4042\n",
      "19/09/29 23:08:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "19/09/29 23:08:11 INFO MemoryStore: MemoryStore cleared\n",
      "19/09/29 23:08:11 INFO BlockManager: BlockManager stopped\n",
      "19/09/29 23:08:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "19/09/29 23:08:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "19/09/29 23:08:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "19/09/29 23:08:12 INFO ShutdownHookManager: Shutdown hook called\n",
      "19/09/29 23:08:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b/pyspark-aa39f3a3-6ce2-4867-93f8-d9263cdfdd39\n",
      "19/09/29 23:08:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-fde62905-a45a-4098-8a74-665f7bc3ca71\n",
      "19/09/29 23:08:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/spark-73de1c97-8ddd-4647-b8af-f93383578e6b\n"
     ]
    }
   ],
   "source": [
    "! spark-submit \\\n",
    "--packages org.apache.spark:spark-avro_2.11:2.4.3 \\\n",
    "pyfiles/avro_read.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDBC\n",
    "\n",
    "All installed jars can be found here:\n",
    "\n",
    "    ll /Users/esn/anaconda3/lib/python3.7/site-packages/pyspark/jars/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyfiles/jdbc_read.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyfiles/jdbc_read.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "spark.read.format('jdbc').\\\n",
    "options(url='jdbc:sqlite:data/my-sqlite.db',\\\n",
    "dbtable='flight_info',driver='org.sqlite.JDBC').load().show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/09/25 21:29:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/09/25 21:29:08 INFO SparkContext: Running Spark version 2.4.4\n",
      "19/09/25 21:29:08 INFO SparkContext: Submitted application: Spark session\n",
      "19/09/25 21:29:08 INFO SecurityManager: Changing view acls to: esn\n",
      "19/09/25 21:29:08 INFO SecurityManager: Changing modify acls to: esn\n",
      "19/09/25 21:29:08 INFO SecurityManager: Changing view acls groups to: \n",
      "19/09/25 21:29:08 INFO SecurityManager: Changing modify acls groups to: \n",
      "19/09/25 21:29:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(esn); groups with view permissions: Set(); users  with modify permissions: Set(esn); groups with modify permissions: Set()\n",
      "19/09/25 21:29:13 INFO Utils: Successfully started service 'sparkDriver' on port 54418.\n",
      "19/09/25 21:29:13 INFO SparkEnv: Registering MapOutputTracker\n",
      "19/09/25 21:29:13 INFO SparkEnv: Registering BlockManagerMaster\n",
      "19/09/25 21:29:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/09/25 21:29:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/09/25 21:29:13 INFO DiskBlockManager: Created local directory at /private/var/folders/p5/qthc5m9n0_d5r5k983p9s69m0000gn/T/blockmgr-d642419a-fd59-4ecc-899e-bfd21683ab1a\n",
      "19/09/25 21:29:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/09/25 21:29:13 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "19/09/25 21:29:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "19/09/25 21:29:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "19/09/25 21:29:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.3.105:4041\n",
      "19/09/25 21:29:14 INFO SparkContext: Added JAR file:///Users/esn/repos/awesomepy-notebooks/spark/drivers/sqlite-jdbc-3.15.1.jar at spark://192.168.3.105:54418/jars/sqlite-jdbc-3.15.1.jar with timestamp 1569439754185\n",
      "19/09/25 21:29:14 INFO Executor: Starting executor ID driver on host localhost\n",
      "19/09/25 21:29:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54419.\n",
      "19/09/25 21:29:14 INFO NettyBlockTransferService: Server created on 192.168.3.105:54419\n",
      "19/09/25 21:29:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/09/25 21:29:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.3.105, 54419, None)\n",
      "19/09/25 21:29:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.3.105:54419 with 366.3 MB RAM, BlockManagerId(driver, 192.168.3.105, 54419, None)\n",
      "19/09/25 21:29:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.3.105, 54419, None)\n",
      "19/09/25 21:29:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.3.105, 54419, None)\n",
      "19/09/25 21:29:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/esn/repos/awesomepy-notebooks/spark/spark-warehouse').\n",
      "19/09/25 21:29:15 INFO SharedState: Warehouse path is 'file:/Users/esn/repos/awesomepy-notebooks/spark/spark-warehouse'.\n",
      "19/09/25 21:29:16 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! spark-submit \\\n",
    "--driver-class-path drivers/sqlite-jdbc-3.15.1.jar \\\n",
    "--jars drivers/sqlite-jdbc-3.15.1.jar \\\n",
    "pyfiles/jdbc_read.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDL formatted schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------+\n",
      "|         DESTINATION|          ORIGIN|NUM_OF_FLIGHTS|\n",
      "+--------------------+----------------+--------------+\n",
      "|       United States|         Romania|           1.0|\n",
      "|       United States|         Ireland|         264.0|\n",
      "|       United States|           India|          69.0|\n",
      "|               Egypt|   United States|          24.0|\n",
      "|   Equatorial Guinea|   United States|           1.0|\n",
      "|       United States|       Singapore|          25.0|\n",
      "|       United States|         Grenada|          54.0|\n",
      "|          Costa Rica|   United States|         477.0|\n",
      "|             Senegal|   United States|          29.0|\n",
      "|       United States|Marshall Islands|          44.0|\n",
      "|              Guyana|   United States|          17.0|\n",
      "|       United States|    Sint Maarten|          53.0|\n",
      "|               Malta|   United States|           1.0|\n",
      "|             Bolivia|   United States|          46.0|\n",
      "|            Anguilla|   United States|          21.0|\n",
      "|Turks and Caicos ...|   United States|         136.0|\n",
      "|       United States|     Afghanistan|           2.0|\n",
      "|Saint Vincent and...|   United States|           1.0|\n",
      "|               Italy|   United States|         390.0|\n",
      "|       United States|          Russia|         156.0|\n",
      "+--------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"DESTINATION VARCHAR(255), ORIGIN VARCHAR(255), NUM_OF_FLIGHTS DOUBLE\"\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .option(\"inferShema\", \"true\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .schema(schema)\n",
    "      .csv('data/2010-summary.csv'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DESTINATION: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- NUM_OF_FLIGHTS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+------------------+\n",
      "|summary|DESTINATION|     ORIGIN|    NUM_OF_FLIGHTS|\n",
      "+-------+-----------+-----------+------------------+\n",
      "|  count|        255|        255|               255|\n",
      "|   mean|       null|       null| 1655.956862745098|\n",
      "| stddev|       null|       null|21801.481975969557|\n",
      "|    min|Afghanistan|Afghanistan|               1.0|\n",
      "|    max|    Vietnam|    Vietnam|          348113.0|\n",
      "+-------+-----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to construct and specify a schema using the StructType classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType()),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType()),\n",
    "    StructField(\"count\", IntegerType())\n",
    "])\n",
    "\n",
    "df = (spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .schema(schema)\n",
    "      .csv('data/2010-summary.csv'))  # json, parquet, ocr\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
