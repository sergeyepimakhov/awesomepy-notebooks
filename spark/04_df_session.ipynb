{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session\n",
    "To get all the \"various Spark parameters as key-value pairs\" for a SparkSession, â€œThe entry point to programming Spark with the Dataset and DataFrame API,\" run the following (this is using spark python api, scala would be very similar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 100)  # custom settings\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.port', '53216'),\n",
       " ('spark.app.id', 'local-1569014349990'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', '192.168.3.105'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Spark session'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.shuffle.partitions', '100')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame/Dataset from a collection (e.g. list or set)\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# dataframe from list\n",
    "mylist = [1, 2, 3, 4]\n",
    "spark.createDataFrame(mylist, IntegerType()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|  1.0|\n",
      "|  2.0|\n",
      "|  3.0|\n",
      "|  4.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# dataframe from set\n",
    "myset = (1., 2., 3., 4.)\n",
    "spark.createDataFrame(myset, FloatType()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 25|\n",
      "|Simon| 41|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create from list of tuples\n",
    "mylist = [('Alice', 34), ('Bob', 25), ('Simon', 41)]\n",
    "\n",
    "spark.createDataFrame(mylist, ['name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esn/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 34|Alice|\n",
      "| 25|  Bob|\n",
      "| 41|Simon|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create from a dictionary\n",
    "mydict = [\n",
    "    {'name': 'Alice', 'age': 34}, \n",
    "    {'name': 'Bob',   'age': 25}, \n",
    "    {'name': 'Simon', 'age': 41}\n",
    "]\n",
    "\n",
    "spark.createDataFrame(mydict).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 34|Alice|\n",
      "| 25|  Bob|\n",
      "| 41|Simon|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the same but without the warning\n",
    "from pyspark.sql import Row\n",
    "mydict = [\n",
    "    {'name': 'Alice', 'age': 34}, \n",
    "    {'name': 'Bob',   'age': 25}, \n",
    "    {'name': 'Simon', 'age': 41}\n",
    "]\n",
    "\n",
    "spark.createDataFrame(Row(**x) for x in mydict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame for a range of numbers\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 10, 2).toDF('number').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the DataFrameReaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|                 _c0|                _c1|  _c2|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "      .csv('data/2010-summary.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register User Defined Functions (UDFs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power2(value):\n",
    "    return value * value\n",
    "\n",
    "df = spark.range(5).toDF('nums')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|nums|\n",
      "+----+\n",
      "|   0|\n",
      "|   1|\n",
      "|   2|\n",
      "|   3|\n",
      "|   4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|power2(nums)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           1|\n",
      "|           4|\n",
      "|           9|\n",
      "|          16|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "power2udf = udf(power2)\n",
    "\n",
    "df.select(power2udf(col('nums'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|power2(nums)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           1|\n",
      "|           4|\n",
      "|           9|\n",
      "|          16|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the UDF for SQL\n",
    "spark.udf.register('power2', power2)\n",
    "\n",
    "df.selectExpr('power2(nums)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
