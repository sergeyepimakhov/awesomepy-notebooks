{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"Spark session\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "      .parquet('data/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|        Afghanistan|    2|\n",
      "|            Algeria|    1|\n",
      "|             Angola|   18|\n",
      "|           Anguilla|   20|\n",
      "|Antigua and Barbuda|  121|\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select and sort some data\n",
    "df_sel = df.select('ORIGIN_COUNTRY_NAME', 'count').distinct().sort(asc('ORIGIN_COUNTRY_NAME'))\n",
    "df_sel.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|first(ORIGIN_COUNTRY_NAME, false)|\n",
      "+---------------------------------+\n",
      "|                      Afghanistan|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "df_sel.agg(first('ORIGIN_COUNTRY_NAME')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|last(ORIGIN_COUNTRY_NAME, false)|\n",
      "+--------------------------------+\n",
      "|                         Vietnam|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last\n",
    "df_sel.agg(last('ORIGIN_COUNTRY_NAME')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# min \n",
    "df_sel.agg(min('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    348113|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max\n",
    "df_sel.agg(max('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|             Russia|  156|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# contain\n",
    "df_sel[df_sel['ORIGIN_COUNTRY_NAME'].like('%ussia%')].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode\n",
    "#\n",
    "# Input:\n",
    "# FieldA    FieldB    ArrayField\n",
    "# 1         A         {1,2,3}\n",
    "# 2         B         {3,5}\n",
    "# \n",
    "# Result:\n",
    "# FieldA    FieldB    ExplodedField\n",
    "# 1         A         1\n",
    "# 1         A         2\n",
    "# 1         A         3\n",
    "# 2         B         3\n",
    "# 2         B         5\n",
    "\n",
    "df_explode = spark.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])\n",
    "df_explode.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   A|   1|\n",
      "|   1|   A|   2|\n",
      "|   1|   A|   3|\n",
      "|   2|   B|   3|\n",
      "|   2|   B|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_explode.withColumn(\"col3\", explode(df_explode.col3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                data|\n",
      "+--------------------+\n",
      "|[[1, 2, 3], [4, 5...|\n",
      "|          [, [4, 5]]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flatten\n",
    "# create single array from array of arrays \n",
    "df_flatten = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
    "df_flatten.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     flatten(data)|\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "|              null|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flatten.select(flatten(df_flatten.data)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date time functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|input_date|     input_timestamp|\n",
      "+----------+--------------------+\n",
      "|2019-01-23|2019-01-23 01:23:...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row \n",
    "\n",
    "# yyyy-MM-dd or yyyy-MM-dd HH:mm:ss.SSSS format\n",
    "dtimes = spark.sparkContext.parallelize([Row(input_date='2019-01-23', input_timestamp='2019-01-23 01:23:45.6789')]).toDF()\n",
    "dtimes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2019-10-03|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current date\n",
    "dtimes.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input_date: string (nullable = true)\n",
      " |-- date_format(input_date, yyyy/MM/dd): string (nullable = true)\n",
      " |-- date_format(input_timestamp, yyyy/MM/dd HH:mm:ss.SSSS): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date format\n",
    "dtimes.select('input_date',\n",
    "              date_format('input_date', 'yyyy/MM/dd'),\n",
    "              date_format('input_timestamp', 'yyyy/MM/dd HH:mm:ss.SSSS')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------+------------------------------------------------------+\n",
      "|input_date|date_format(input_date, yyyy/MM/dd)|date_format(input_timestamp, yyyy/MM/dd HH:mm:ss.SSSS)|\n",
      "+----------+-----------------------------------+------------------------------------------------------+\n",
      "|2019-01-23|                         2019/01/23|                                  2019/01/23 01:23:...|\n",
      "+----------+-----------------------------------+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date format\n",
    "dtimes.select('input_date',\n",
    "              date_format('input_date', 'yyyy/MM/dd'),\n",
    "              date_format('input_timestamp', 'yyyy/MM/dd HH:mm:ss.SSSS')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input_date: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_date, to_timestamp\n",
    "(dtimes\n",
    " .select('input_date',\n",
    "         'input_timestamp',\n",
    "         to_date('input_date').alias('date'),\n",
    "         to_timestamp('input_timestamp').alias('timestamp'),\n",
    "        )\n",
    " .printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+\n",
      "|input_date|     input_timestamp|      date|           timestamp|\n",
      "+----------+--------------------+----------+--------------------+\n",
      "|2019-01-23|2019-01-23 01:23:...|2019-01-23|2019-01-23 01:23:...|\n",
      "+----------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dtimes\n",
    " .select('input_date',\n",
    "         'input_timestamp',\n",
    "         to_date('input_date').alias('date'),\n",
    "         to_timestamp('input_timestamp').alias('timestamp'),\n",
    "        )\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input_date: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- timestamp_string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# string -> date -> string, string -> timestamp -> string, \n",
    "(dtimes\n",
    " .select('input_date',\n",
    "         'input_timestamp',\n",
    "         to_date('input_date').alias('date'),\n",
    "         to_timestamp('input_timestamp').alias('timestamp'),\n",
    "        )\n",
    " .select('input_date',\n",
    "        'input_timestamp',\n",
    "        'date',\n",
    "        'timestamp',\n",
    "        date_format('date', 'yyyy/MM/dd').alias('date_string'),\n",
    "        date_format('timestamp', 'yyyy/MM/dd HH:mm:ss.SSSS').alias('timestamp_string'))\n",
    " .printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+-----------+--------------------+\n",
      "|input_date|     input_timestamp|      date|           timestamp|date_string|    timestamp_string|\n",
      "+----------+--------------------+----------+--------------------+-----------+--------------------+\n",
      "|2019-01-23|2019-01-23 01:23:...|2019-01-23|2019-01-23 01:23:...| 2019/01/23|2019/01/23 01:23:...|\n",
      "+----------+--------------------+----------+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(dtimes\n",
    " .select('input_date',\n",
    "         'input_timestamp',\n",
    "         to_date('input_date').alias('date'),\n",
    "         to_timestamp('input_timestamp').alias('timestamp'),\n",
    "        )\n",
    " .select('input_date',\n",
    "        'input_timestamp',\n",
    "        'date',\n",
    "        'timestamp',\n",
    "        date_format('date', 'yyyy/MM/dd').alias('date_string'),\n",
    "        date_format('timestamp', 'yyyy/MM/dd HH:mm:ss.SSSS').alias('timestamp_string'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            input|\n",
      "+-----------------+\n",
      "|                1|\n",
      "|                0|\n",
      "|3.141592653589793|\n",
      "|             null|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row \n",
    "from math import pi\n",
    "\n",
    "dmath = spark.sparkContext.parallelize([Row(input='1'),\n",
    "                                       Row(input='0'),\n",
    "                                       Row(input=pi),\n",
    "                                       Row(input=None),\n",
    "                                       ]).toDF()\n",
    "dmath.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|            input|        COS(input)|\n",
      "+-----------------+------------------+\n",
      "|                1|0.5403023058681398|\n",
      "|                0|               1.0|\n",
      "|3.141592653589793|              -1.0|\n",
      "|             null|              null|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cos\n",
    "dmath.select('input',\n",
    "           cos(col('input'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|            input|FLOOR(input)|\n",
      "+-----------------+------------+\n",
      "|                1|           1|\n",
      "|                0|           0|\n",
      "|3.141592653589793|           3|\n",
      "|             null|        null|\n",
      "+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# floor\n",
    "dmath.select('input',\n",
    "           floor(col('input'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|            input|        LOG(input)|\n",
      "+-----------------+------------------+\n",
      "|                1|               0.0|\n",
      "|                0|              null|\n",
      "|3.141592653589793|1.1447298858494002|\n",
      "|             null|              null|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log\n",
    "dmath.select('input',\n",
    "           log(col('input'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Functions (hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(crc32=2743272264)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crc32\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(md5='902fbdd2b1df0c4f70b4a5d23525e932')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# md5\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('md5')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sha1='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sha1\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('sha1')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sha2='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sha2\n",
    "spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('sha2')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "| ABC|NaN|\n",
      "|null|1.0|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ABC', float('nan')), \n",
    "                            (None, 1.0)], \n",
    "                           (\"a\", \"b\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|isnan(a)|isnan(b)|\n",
      "+--------+--------+\n",
      "|   false|    true|\n",
      "|   false|   false|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# isnan\n",
    "df.select(isnan('a'), isnan('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|(a IS NULL)|(b IS NULL)|\n",
      "+-----------+-----------+\n",
      "|      false|      false|\n",
      "|       true|      false|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# isnull\n",
    "df.select(isnull('a'), isnull('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|null|1.0|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(isnull('a')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|null|1.0|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('a').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|ABC|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('a') == col('a')).show() # :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|ABC|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(~isnull('a')).show() # not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|ABC|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('a').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|null|1.0|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('b') != 'NaN').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|ABC|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(isnan('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|   a|  b|\n",
      "+----+---+\n",
      "|null|1.0|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(~isnan('b')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    2|\n",
      "|    3|\n",
      "| null|\n",
      "|    1|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict = [\n",
    "    {'value': 2}, \n",
    "    {'value': 3},\n",
    "    {'value': None},  \n",
    "    {'value': 1},\n",
    "    {'value': 5},\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(dict)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| null|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asc\n",
    "df.sort(asc('value')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    5|\n",
      "|    3|\n",
      "|    2|\n",
      "|    1|\n",
      "| null|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# desc\n",
    "df.sort(desc('value')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| null|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderBy is alias for sort function\n",
    "df.orderBy(asc('value')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    5|\n",
      "|    3|\n",
      "|    2|\n",
      "|    1|\n",
      "| null|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc('value')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| null|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('value')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    5|\n",
      "|    3|\n",
      "|    2|\n",
      "|    1|\n",
      "| null|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('value'), ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 35|Alice|\n",
      "| 23|  Bob|\n",
      "| 46|Cindy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dic = [\n",
    "    {'name': 'Alice',\n",
    "     'age': 35},\n",
    "    {'name': 'Bob',\n",
    "     'age': 23},\n",
    "    {'name': 'Cindy',\n",
    "     'age': 46},\n",
    "      ]\n",
    "\n",
    "df = spark.createDataFrame(dic)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 23| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# match\n",
    "df.filter(df.name.like('%ob%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 23|  Bob|\n",
      "| 46|Cindy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in range\n",
    "df.filter(df.name.isin('Bob', 'Cindy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "|age| name|      res|\n",
      "+---+-----+---------+\n",
      "| 35|Alice|Alice35yo|\n",
      "| 23|  Bob|  Bob23yo|\n",
      "| 46|Cindy|Cindy46yo|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concat\n",
    "df.withColumn('res', concat(df.name, df.age, lit('yo'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "|age| name| for_trim|\n",
      "+---+-----+---------+\n",
      "| 35|Alice|  Alice  |\n",
      "| 23|  Bob|    Bob  |\n",
      "| 46|Cindy|  Cindy  |\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('for_trim', concat(lit('  '), df.name, lit('  ')))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      r|\n",
      "+-------+\n",
      "|Alice  |\n",
      "|  Bob  |\n",
      "|Cindy  |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left trim\n",
    "df.select(ltrim(df.for_trim).alias('r')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      r|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "|  Cindy|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# right trim\n",
    "df.select(rtrim(df.for_trim).alias('r')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|    r|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "|Cindy|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trim\n",
    "df.select(trim(df.for_trim).alias('r')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('for_trim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 23| Bob|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select\n",
    "df.filter(df['name'].rlike('.ob$')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|age| name|  res|\n",
      "+---+-----+-----+\n",
      "| 35|Alice|Alice|\n",
      "| 23|  Bob|  BOB|\n",
      "| 46|Cindy|Cindy|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "df.withColumn('res', regexp_replace('name', 'ob', 'OB')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='100')]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regex extract\n",
    "df = spark.createDataFrame([('100-200',)], ['str'])\n",
    "df.select(regexp_extract('str', '(\\d+)-(\\d+)', 1).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(r='200')]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(regexp_extract('str', '(\\d+)-(\\d+)', 2).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Functions\n",
    "\n",
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|<lambda>(name)|\n",
      "+--------------+\n",
      "|             5|\n",
      "|             3|\n",
      "|             5|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new UDF\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "\n",
    "dic = [\n",
    "    {'name': 'Alice'},\n",
    "    {'name': 'Bob'},\n",
    "    {'name': 'Cindy'},\n",
    "      ]\n",
    "\n",
    "df = spark.createDataFrame(dic)\n",
    "df.select(slen('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "https://stackoverflow.com/questions/41661068/group-by-rank-and-aggregate-spark-data-frame-using-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
